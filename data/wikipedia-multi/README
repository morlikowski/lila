This folder contains the wikipedia-multi dataset. It was developed
for research into language identification of multilingual documents.
It is a synthetic dataset, developed from a heavily-normalized version
of wikipedia (dubbed wikicontent).

The set of languages we use is specified in selected_languages. We select
44 languages, which are the intersection of languages with >1000 documents 
in wikicontent and the 97 languages covered by langid.py. The languages
are listed in `selected_langs`.

We generate a total of 12000 documents in 4 logical partitions:
  docsMR(5000) - Monolingual tRaining
  docsME(1000) - Monolingual tEst
  docsUR(5000) - mUltilingual tRaining
  docsUE(1000) - mUltilingual tEst

For the multilingual documents, we generate equal numbers of documents for 
each document order. Document order refers to the number of languages present
in a single document. Order 3 means that there are 3 different languages present 
in the document. Thus, the multilingual sets are further divided into 5 directories.
`docsUE2` thus holds 200 order-2 multilingual documents.

Each document directory is generated through one call to generate.main.
The process is independent for each document. It is roughly as follows:

 1) select N languages without replacement
 2) select a document for each language (again without replacement)
 3) take the top 1/N section of the document (by number of lines, we do not break lines)
 4) join the N sections (no additional space introducted)

For the selection of languages, generate.py supports two different parameters: (1) 'uniform', which specifies a uniform selection 
probability over languages, and (2) 'natural', which respects the natural skew of the training data.  Due to limited amounts of data in 
lesser-used languages, the 'natural' distribution only uses ~16 languages to sample 10000 documents, and so we have only used 'uniform' as 
the language prior skew.

For each docs{M,U}{R,E} folder containing the generated documents, there are two
metadata files. The first is docs*-used, and lists the raw wikicontent documents
that were used to produce the generated documents. These files are used to track 
which documents were used in the generation process, and serve as evidence that 
there is no overlap between the sections.

The second file is docs*-meta, and contains metadata for each document in the
following format:

  filename,part,total_parts,part_language,number of bytes

For example:

  doc1,1,1,en,192
  doc1,2,2,de,67
  doc2,1,1,en,424
  ...

The entire wikipedia-multi dataset can be generated by invoking `build.py`.
The available documents must be listed in advance, in a file called `docs_available` by
default. While we do not assume use of the wikicontent dataset, generate.py does assume
that the documents are in fragment-per-line format.

Finally, paritioning is carried out by partition.py. We initially declare 4 partitionings,
corresponding to the cross product of training and testing on monolingual and multilingual
documents. The partitions are explicitly named and stored in the directory `partitions`.


Marco Lui, July 2012
